{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fa9b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\synnc\\miniconda3\\envs\\kws\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\synnc\\miniconda3\\envs\\kws\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\synnc\\miniconda3\\envs\\kws\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\synnc\\miniconda3\\envs\\kws\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\synnc\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # CUDA/CPU 등 자동 감지\n",
    "model_id = \"gpt2\"  # ← GPT-2 small\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e23a044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 2304])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fe8fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits size: torch.Size([2, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =============================================================================\n",
    "# 1) GPT2Config: 모델 하이퍼파라미터를 저장하는 클래스\n",
    "# =============================================================================\n",
    "class GPT2Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,\n",
    "        hidden_size=768,            # 임베딩 및 히든 차원\n",
    "        n_layer=12,                 # Transformer 블록 개수\n",
    "        n_head=12,                  # 멀티헤드 어텐션 헤드 수\n",
    "        max_position_embeddings=1024,\n",
    "        resid_pdrop=0.1,            # Residual Dropout 확률\n",
    "        attn_pdrop=0.1,             # Attention Dropout 확률\n",
    "        embd_pdrop=0.1,             # Embedding Dropout 확률\n",
    "        layer_norm_epsilon=1e-5\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) CausalSelfAttention: GPT-2의 인과적(Self-masked) 멀티헤드 어텐션\n",
    "# =============================================================================\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        assert config.hidden_size % config.n_head == 0, \\\n",
    "            \"hidden_size must be divisible by n_head.\"\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = config.hidden_size // config.n_head\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "        # 1×1 Conv1d 형태로 Q, K, V를 동시에 생성\n",
    "        self.c_attn = nn.Conv1d(\n",
    "            in_channels=config.hidden_size,\n",
    "            out_channels=3 * config.hidden_size,\n",
    "            kernel_size=1,\n",
    "            bias=True\n",
    "        )\n",
    "        # 어텐션 결과를 다시 hidden_size 차원으로 projection\n",
    "        self.c_proj = nn.Conv1d(\n",
    "            in_channels=config.hidden_size,\n",
    "            out_channels=config.hidden_size,\n",
    "            kernel_size=1,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        # 인과적 마스킹을 위한 버퍼: (1, 1, max_seq, max_seq)\n",
    "        # <prefix, prefix, i, j> 쌍 중 i < j인 경우 -inf 마스크\n",
    "        mask = torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings))\n",
    "        # mask[i, j] = 1 if i >= j else 0\n",
    "        self.register_buffer(\"mask\", mask.view(1, 1, config.max_position_embeddings, config.max_position_embeddings))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, hidden_size)\n",
    "        리턴: (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()  # batch, time, hidden\n",
    "\n",
    "        # 1) Conv1d 레이어를 쓰기 위해 (B, C, T)로 차원 변환\n",
    "        x = x.transpose(1, 2)                   # → (B, hidden_size, T)\n",
    "        qkv = self.c_attn(x)                    # → (B, 3*hidden_size, T)\n",
    "        qkv = qkv.reshape(B, 3, C, T)           # → (B, 3, hidden_size, T)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]  # 각각 (B, hidden_size, T)\n",
    "\n",
    "        # 2) (B, hidden_size, T) → multi-head로 분리\n",
    "        #    각 head마다 (B, n_head, head_dim, T)\n",
    "        q = q.view(B, self.n_head, self.head_dim, T)\n",
    "        k = k.view(B, self.n_head, self.head_dim, T)\n",
    "        v = v.view(B, self.n_head, self.head_dim, T)\n",
    "\n",
    "        # 3) 전치해서 (B, n_head, T, head_dim) 형태로 변경 (행렬 곱용)\n",
    "        q = q.permute(0, 1, 3, 2)  # (B, n_head, T, head_dim)\n",
    "        k = k.permute(0, 1, 3, 2)  # (B, n_head, T, head_dim)\n",
    "        v = v.permute(0, 1, 3, 2)  # (B, n_head, T, head_dim)\n",
    "\n",
    "        # 4) attention score 계산: (B, n_head, T, T)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # 5) 마스킹: 미래 토큰 참조를 막기 위해 하삼각행렬(인과 마스크) 곱함\n",
    "        #    mask[:, :, :T, :T] → (1,1,T,T)\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # 6) softmax 후 dropout\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)       # (B, n_head, T, T)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "\n",
    "        # 7) attention 결과: (B, n_head, T, head_dim)\n",
    "        attn_output = torch.matmul(attn_probs, v)\n",
    "\n",
    "        # 8) (B, n_head, T, head_dim) → (B, hidden_size, T)\n",
    "        attn_output = attn_output.permute(0, 1, 3, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, C, T)\n",
    "\n",
    "        # 9) projection + dropout → (B, hidden_size, T)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        # 10) (B, hidden_size, T) → (B, T, hidden_size)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) MLP (Feed-Forward Network)\n",
    "# =============================================================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Conv1d(\n",
    "            in_channels=config.hidden_size,\n",
    "            out_channels=4 * config.hidden_size,  # hidden → 4×hidden\n",
    "            kernel_size=1,\n",
    "            bias=True\n",
    "        )\n",
    "        self.c_proj = nn.Conv1d(\n",
    "            in_channels=4 * config.hidden_size,\n",
    "            out_channels=config.hidden_size,\n",
    "            kernel_size=1,\n",
    "            bias=True\n",
    "        )\n",
    "        self.act = nn.GELU()  # GPT-2의 경우에도 GELU 활성화 사용\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, hidden_size)\n",
    "        리턴: (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 1) (B, T, hidden) → (B, hidden, T)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # 2) Fully-connected (1×1 Conv) + GELU → (B, 4*hidden, T)\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        # 3) 다시 projection: (B, 4*hidden, T) → (B, hidden, T)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 4) (B, hidden, T) → (B, T, hidden)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4) TransformerBlock: GPT-2 블록 (Self-Attention + MLP + Residual + LayerNorm)\n",
    "# =============================================================================\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, hidden_size)\n",
    "        리턴: (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # 1) 첫 번째 LayerNorm + Self-Attention + Residual\n",
    "        x_norm1 = self.ln_1(x)\n",
    "        attn_out = self.attn(x_norm1)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # 2) 두 번째 LayerNorm + MLP + Residual\n",
    "        x_norm2 = self.ln_2(x)\n",
    "        mlp_out = self.mlp(x_norm2)\n",
    "        x = x + mlp_out\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5) GPT2Model: 전체 GPT-2 네트워크\n",
    "# =============================================================================\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1) 토큰 임베딩 (wte) & 위치 임베딩 (wpe)\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "        # 2) Transformer 블록을 n_layer만큼 쌓음\n",
    "        self.h = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)])\n",
    "\n",
    "        # 3) 마지막 LayerNorm\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # (선택) 출력 토큰 예측을 위해 lm_head 선언\n",
    "        # 만약 GPT2Model 단독으로 사용하려면 이 부분 생략 가능\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 공식 구현과 유사한 방식으로 초기화 (간단히 Normal 분포로만 예시)\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "            else:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, seq_len)  LongTensor (토큰 ID)\n",
    "        position_ids: (batch_size, seq_len) LongTensor (옵션). None인 경우 0~T-1 자동 생성\n",
    "        리턴: logits or hidden_states\n",
    "          - hidden_states: (batch_size, seq_len, hidden_size)\n",
    "          - logits: (batch_size, seq_len, vocab_size)  ← lm_head 사용 시\n",
    "        \"\"\"\n",
    "        B, T = input_ids.size()\n",
    "        device = input_ids.device\n",
    "\n",
    "        # 1) position_ids 생성 (없을 경우 0~T-1)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(T, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(B, T)  # (B, T)\n",
    "\n",
    "        # 2) 토큰 임베딩 + 위치 임베딩 → (B, T, hidden_size)\n",
    "        token_embeddings = self.wte(input_ids)      # (B, T, hidden)\n",
    "        pos_embeddings = self.wpe(position_ids)     # (B, T, hidden)\n",
    "        hidden_states = token_embeddings + pos_embeddings\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        # 3) Transformer 블록 순차 적용\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states)    # (B, T, hidden)\n",
    "\n",
    "        # 4) 최종 LayerNorm\n",
    "        hidden_states = self.ln_f(hidden_states)    # (B, T, hidden)\n",
    "\n",
    "        # 5) lm_head를 거쳐서 최종 logits 반환 (옵션)\n",
    "        logits = self.lm_head(hidden_states)        # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6) 사용 예시\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) config 정의\n",
    "    config = GPT2Config(\n",
    "        vocab_size=50257,\n",
    "        hidden_size=768,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        max_position_embeddings=1024\n",
    "    )\n",
    "    # 2) 모델 생성\n",
    "    model = GPT2Model(config)\n",
    "\n",
    "    # 3) 더미 입력 (batch_size=2, seq_len=10)\n",
    "    dummy_input = torch.randint(0, config.vocab_size, (2, 10), dtype=torch.long)\n",
    "    dummy_position = None  # None이면 forward 내부에서 자동 생성\n",
    "\n",
    "    # 4) 순방향 실행\n",
    "    logits = model(dummy_input, dummy_position)  # (2, 10, 50257)\n",
    "    print(\"Logits size:\", logits.size())         # → torch.Size([2, 10, 50257])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6604b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_block = TransformerBlock(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59925b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): CausalSelfAttention(\n",
       "    (c_attn): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
       "    (c_proj): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): MLP(\n",
       "    (c_fc): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
       "    (c_proj): Conv1d(3072, 768, kernel_size=(1,), stride=(1,))\n",
       "    (act): GELU(approximate='none')\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_block.ln_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcc93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(1, 128, 768)\n",
    "y=sample_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e29ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# layer norm 1\n",
    "after_ln_1=(x-x.mean(dim=-1, keepdim=True))/torch.sqrt(x.var(dim=-1, keepdim=True)+1e-5)*sample_block.ln_1.weight + sample_block.ln_1.bias\n",
    "\n",
    "# c_attn\n",
    "after_c_attn = F.conv1d(after_ln_1.permute(0,2,1), sample_block.attn.c_attn.weight,sample_block.attn.c_attn.bias )\n",
    "q=after_c_attn[:, 0:768,:]\n",
    "k=after_c_attn[:, 768:768*2, :]\n",
    "v=after_c_attn[:, 768*2:768*3, :]\n",
    "batch_size, _, seq_len = q.shape  # q.shape = (1, 768, 128)\n",
    "num_heads  = 12\n",
    "head_dim    = 768 // num_heads \n",
    "q_reshaped = q.view(batch_size, num_heads, head_dim, seq_len)\n",
    "k_reshaped = k.view(batch_size, num_heads, head_dim, seq_len)\n",
    "q_for_matmul = q_reshaped.permute(0, 1, 3, 2)  \n",
    "\n",
    "# Q @ K.T\n",
    "attn_scores = torch.matmul(q_for_matmul, k_reshaped)  \n",
    "attn_scores = attn_scores / math.sqrt(head_dim)\n",
    "attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "v_reshaped = v.view(batch_size, num_heads, head_dim, seq_len).permute(0, 1, 3, 2) \n",
    "# score @ V\n",
    "context_permuted=torch.matmul(attn_probs, v_reshaped)\n",
    "context_final = context_permuted.contiguous().view(batch_size, seq_len, num_heads*head_dim) #1, 128, 768\n",
    "context_for_cproj = context_final.permute(0, 2, 1)  # → (1, 768, 128)\n",
    "\n",
    "#c_proj\n",
    "out = sample_block.attn.c_proj(context_for_cproj)   # → (1, 768, 128)\n",
    "out = out.permute(0, 2, 1)                           # → (1, 128, 768)\n",
    "\n",
    "#layer norm 2\n",
    "after_ln_2=(out-out.mean(dim=-1, keepdim=True))/torch.sqrt(out.var(dim=-1, keepdim=True)+1e-5)*sample_block.ln_2.weight + sample_block.ln_2.bias\n",
    "\n",
    "# mlp c_fc\n",
    "after_c_fc= sample_block.mlp.c_fc(after_ln_2.permute(0, 2, 1))\n",
    "\n",
    "#GeLU\n",
    "after_act = sample_block.mlp.act(after_c_fc)\n",
    "\n",
    "# mlp c_proj\n",
    "after_c_proj = sample_block.mlp.c_proj(after_act)\n",
    "after_mlp = after_c_proj.permute(0, 2, 1)\n",
    "\n",
    "# residual connection\n",
    "output = after_ln_2 + after_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sample_block = TransformerBlock(config=config)\n",
    "\n",
    "batch_size = 1\n",
    "num_heads  = config.num_attention_heads  # 12\n",
    "head_dim   = config.hidden_size // num_heads  # 64\n",
    "hidden_dim = config.hidden_size         # 768\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# KV 캐시: 처음에는 길이가 0인 텐서 (shape=(B, num_heads, head_dim, 0))\n",
    "k_cache = torch.zeros(batch_size, num_heads, head_dim, 0, device=device)\n",
    "v_cache = torch.zeros(batch_size, num_heads, head_dim, 0, device=device)\n",
    "\n",
    "# 10개의 토큰을 하나씩 처리\n",
    "# x_tensors = [embedding_of_token1, embedding_of_token2, ...]  \n",
    "# 여기서는 임의의 벡터로 대체\n",
    "x_tensors = [torch.randn(batch_size, 1, hidden_dim, device=device) for _ in range(10)]\n",
    "\n",
    "outputs = []  # 매 스텝의 최종 output(b,1,hidden)을 저장하기 위해\n",
    "\n",
    "for t, x_t in enumerate(x_tensors, start=1):\n",
    "    # x_t.shape == (batch_size, 1, hidden_dim)\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (A) 1차 LayerNorm → Q/K/V 계산\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    after_ln_1 = sample_block.ln_1(x_t)            # → (B, 1, 768)\n",
    "    ln1_for_conv = after_ln_1.permute(0, 2, 1)      # → (B, 768, 1)\n",
    "    c_attn_out   = sample_block.attn.c_attn(ln1_for_conv)\n",
    "    # c_attn_out.shape == (B, 3*hidden=2304, 1)\n",
    "\n",
    "    # Q_new / K_new / V_new 분리\n",
    "    q_new = c_attn_out[:,    0:hidden_dim,     :]  # → (B, 768, 1)\n",
    "    k_new = c_attn_out[:,  hidden_dim:2*hidden_dim, :]  # → (B, 768, 1)\n",
    "    v_new = c_attn_out[:, 2*hidden_dim:3*hidden_dim, :]  # → (B, 768, 1)\n",
    "\n",
    "    # Head 단위로 reshape: (B, 768, 1) → (B, num_heads, head_dim, 1)\n",
    "    q_new_reshaped = q_new.view(batch_size, num_heads, head_dim, 1)  # (B,12,64,1)\n",
    "    k_new_reshaped = k_new.view(batch_size, num_heads, head_dim, 1)  # (B,12,64,1)\n",
    "    v_new_reshaped = v_new.view(batch_size, num_heads, head_dim, 1)  # (B,12,64,1)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (B) KV 캐시 업데이트\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    #   k_cache, v_cache 는 이전 스텝에서 이미 만들어진 (B,12,64,t-1)짜리 상태\n",
    "    #   새 k_new_reshaped, v_new_reshaped(각각 (B,12,64,1))을 뒤에 붙이기\n",
    "    k_cache = torch.cat([k_cache, k_new_reshaped], dim=-1)  # → (B,12,64,t)\n",
    "    v_cache = torch.cat([v_cache, v_new_reshaped], dim=-1)  # → (B,12,64,t)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (C) Scaled Dot-Product Attention (한 토큰용)\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (C1) Q·Kᵀ → Score\n",
    "    #   q_new_reshaped: (B,12,64,1) → (B,12,1,64) 으로 permute\n",
    "    q_for_matmul = q_new_reshaped.permute(0, 1, 3, 2)         # → (B,12,1,64)\n",
    "\n",
    "    #   K_full = k_cache: (B,12,64,t)\n",
    "    #   attn_scores = (B,12,1,64) @ (B,12,64,t) → (B,12,1,t)\n",
    "    attn_scores = torch.matmul(q_for_matmul, k_cache)        # → (B,12,1,t)\n",
    "    attn_scores = attn_scores / math.sqrt(head_dim)          # scale\n",
    "\n",
    "    # (C2) Softmax → Attention Weights\n",
    "    #   (B,12,1,t) 에 대해 마지막 차원 t (과거+현재 토큰 개수) 에 대해 softmax\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)          # → (B,12,1,t)\n",
    "\n",
    "    # (C3) Attention Weights @ V_full\n",
    "    #   v_cache: (B,12,64,t) → permute → (B,12,t,64)\n",
    "    v_for_matmul = v_cache.permute(0, 1, 3, 2)                # → (B,12,t,64)\n",
    "\n",
    "    #   context = (B,12,1,t) @ (B,12,t,64) → (B,12,1,64)\n",
    "    context = torch.matmul(attn_probs, v_for_matmul)         # → (B,12,1,64)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (D) Head 합치고 c_proj → Attention 출력\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (D1) (B,12,1,64) → (B,1,12,64)\n",
    "    context_permuted = context.permute(0, 2, 1, 3)            # → (B,1,12,64)\n",
    "\n",
    "    # (D2) (B,1,12,64) → (B,1, 12*64=768)\n",
    "    context_final = context_permuted.contiguous().view(batch_size, 1, num_heads * head_dim)\n",
    "    # context_final.shape == (B,1,768)\n",
    "\n",
    "    # (D3) c_proj 적용\n",
    "    context_for_cproj = context_final.permute(0, 2, 1)        # → (B,768,1)\n",
    "    attn_out = sample_block.attn.c_proj(context_for_cproj)   # → (B,768,1)\n",
    "    attn_out = attn_out.permute(0, 2, 1)                      # → (B,1,768)\n",
    "\n",
    "    # (D4) Residual + LayerNorm₂\n",
    "    #    residual_attn = x_t + attn_out  (shape=(B,1,768))\n",
    "    residual_attn = x_t + attn_out\n",
    "\n",
    "    # LayerNorm₂: hidden 축(=마지막 차원) 기준으로 정규화\n",
    "    μ2   = residual_attn.mean(dim=-1, keepdim=True)                            # (B,1,1)\n",
    "    σ2   = residual_attn.var(dim=-1, keepdim=True, unbiased=False)             # (B,1,1)\n",
    "    norm = (residual_attn - μ2) / torch.sqrt(σ2 + 1e-5)                         # (B,1,768)\n",
    "    after_ln_2 = norm * sample_block.ln_2.weight + sample_block.ln_2.bias      # (B,1,768)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (E) MLP: c_fc → GELU(act) → c_proj → Residual\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # (E1) c_fc (hidden→4*hidden)\n",
    "    mlp_in = after_ln_2.permute(0, 2, 1)                 # → (B,768,1)\n",
    "    after_c_fc = sample_block.mlp.c_fc(mlp_in)           # → (B,3072,1)\n",
    "\n",
    "    # (E2) 활성화(GELU)\n",
    "    after_act_fc = sample_block.mlp.act(after_c_fc)      # → (B,3072,1)\n",
    "\n",
    "    # (E3) c_proj (4*hidden→hidden)\n",
    "    after_c_proj = sample_block.mlp.c_proj(after_act_fc) # → (B,768,1)\n",
    "\n",
    "    # (E4) 다시 (batch, 1, hidden)\n",
    "    after_mlp = after_c_proj.permute(0, 2, 1)             # → (B,1,768)\n",
    "\n",
    "    # (E5) MLP Residual 연결 → 최종 출력\n",
    "    output_t = after_ln_2 + after_mlp                     # → (B,1,768)\n",
    "\n",
    "    # (F) 저장 및 다음 토큰 준비\n",
    "    outputs.append(output_t)  # 이 output_t를 다음 레이어나 혹은 최종분류층 등에 사용할 수 있음\n",
    "\n",
    "    # - 다음 루프(iteration)에서 x_t ← new embedding(예: LM head의 argmax 결과) 형태로 바꿔 넣으면,\n",
    "    #   키/값 캐시가 갱신된 상태에서 다음 토큰 예측이 이어집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1901fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732e548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 128])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5037c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b27598",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_c_fc= sample_block.mlp.c_fc(after_ln_2.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc146eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282922e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_c_proj = sample_block.mlp.c_proj(after_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10d13d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_mlp = after_c_proj.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa0ecae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = after_ln_2 + after_mlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

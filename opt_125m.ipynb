{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bf5ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(profile='full')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0870028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/myenv/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/envs/myenv/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "out shape: torch.Size([1, 5, 50272])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read)))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================================\n",
    "# 1) OPTConfig: 모델 하이퍼파라미터 저장\n",
    "# ============================================================================\n",
    "class OPTConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50272,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        max_position_embeddings=2048,\n",
    "        ffn_dim=None,\n",
    "        dropout=0.1,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.ffn_dim = ffn_dim or 4 * hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2) OPTAttention: 분리형 Q/K/V 기반의 causal self-attention\n",
    "# ============================================================================\n",
    "class OPTAttention(nn.Module):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Q, K, V projections (bias=True로 변경)\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "        # output projection\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # causal mask buffer\n",
    "        mask = torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.size()\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        # reshape to heads\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) # B, num_heads, T, head_dim\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # attention scores\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale # B, num_heads, T, head_dim @ B, num_heads, head_dim, T\n",
    "        causal = self.mask[:T, :T].unsqueeze(0).unsqueeze(0)\n",
    "        attn_scores = attn_scores.masked_fill(causal == 0, float('-inf'))\n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores + attention_mask\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        out = attn_probs @ v\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3) FeedForward (MLP)\n",
    "# ============================================================================\n",
    "class OPTFeedForward(nn.Module):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.ffn_dim, bias=True)\n",
    "        self.activation = F.silu\n",
    "        self.fc2 = nn.Linear(config.ffn_dim, config.hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4) OPTDecoderLayer: 한 레이어\n",
    "# ============================================================================\n",
    "class OPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = OPTAttention(config)\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.feed_forward = OPTFeedForward(config)\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        r = x\n",
    "        x = self.layernorm1(x)\n",
    "        x = self.self_attn(x, attention_mask)\n",
    "        x = x + r\n",
    "        r = x\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        return x + r\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5) OPTModel: 임베딩 + 디코더 스택\n",
    "# ============================================================================\n",
    "class OPTModel(nn.Module):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, T = input_ids.size()\n",
    "        device = input_ids.device\n",
    "        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        return self.final_layernorm(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6) OPTForCausalLM: LM 헤드 (bias=True로 변경)\n",
    "# ============================================================================\n",
    "class OPTForCausalLM(nn.Module):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__()\n",
    "        self.model = OPTModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden = self.model(input_ids, attention_mask)\n",
    "        return self.lm_head(hidden)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7) 가중치 로드 및 테스트\n",
    "# ============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # config 생성\n",
    "    config = OPTConfig()\n",
    "    from transformers import OPTForCausalLM as HF_OPT, OPTConfig as HFConfig\n",
    "\n",
    "    # Hugging Face 모델과 체크포인트\n",
    "    hf_cfg = HFConfig.from_pretrained('facebook/opt-125m')\n",
    "    hf_model = HF_OPT.from_pretrained('facebook/opt-125m', config=hf_cfg)\n",
    "    hf_state = hf_model.state_dict()\n",
    "\n",
    "    # 커스텀 모델 생성\n",
    "    model = OPTForCausalLM(config)\n",
    "\n",
    "    # 새로운 state_dict 매핑 및 버퍼 추가\n",
    "    new_state = {}\n",
    "    for k, v in hf_state.items():\n",
    "        if k == 'model.decoder.embed_tokens.weight':\n",
    "            new_state['model.token_embedding.weight'] = v.clone()\n",
    "        elif k == 'model.decoder.embed_positions.weight':\n",
    "            new_state['model.position_embedding.weight'] = v[:config.max_position_embeddings].clone()\n",
    "        elif k.startswith('model.decoder.layers.'):\n",
    "            parts = k.split('.')\n",
    "            idx = parts[3]\n",
    "            sub = parts[4:]\n",
    "            base = f'model.layers.{idx}'\n",
    "            if sub[0] == 'self_attn':\n",
    "                name = sub[1] + '.' + sub[2]\n",
    "                new_state[f'{base}.self_attn.{name}'] = v.clone()\n",
    "            elif sub[0] == 'self_attn_layer_norm':\n",
    "                new_state[f'{base}.layernorm1.{sub[1]}'] = v.clone()\n",
    "            elif sub[0] == 'fc1':\n",
    "                new_state[f'{base}.feed_forward.fc1.{sub[1]}'] = v.clone()\n",
    "            elif sub[0] == 'fc2':\n",
    "                new_state[f'{base}.feed_forward.fc2.{sub[1]}'] = v.clone()\n",
    "            elif sub[0] == 'final_layer_norm':\n",
    "                new_state[f'{base}.layernorm2.{sub[1]}'] = v.clone()\n",
    "        elif k == 'model.decoder.final_layer_norm.weight':\n",
    "            new_state['model.final_layernorm.weight'] = v.clone()\n",
    "        elif k == 'model.decoder.final_layer_norm.bias':\n",
    "            new_state['model.final_layernorm.bias'] = v.clone()\n",
    "        elif k == 'lm_head.weight':\n",
    "            new_state['lm_head.weight'] = v.clone()\n",
    "        elif k == 'lm_head.bias':\n",
    "            new_state['lm_head.bias'] = v.clone()\n",
    "    # buffer mask\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        mask_buf = model.model.layers[i].self_attn.mask\n",
    "        new_state[f'model.layers.{i}.self_attn.mask'] = mask_buf.clone()\n",
    "\n",
    "    # lm_head.bias가 체크포인트에 없으면 0으로 초기화\n",
    "    if 'lm_head.bias' not in new_state:\n",
    "        new_state['lm_head.bias'] = torch.zeros_like(model.lm_head.bias)\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(new_state, strict=False)\n",
    "    print('Missing keys:', missing)\n",
    "    print('Unexpected keys:', unexpected)\n",
    "\n",
    "    # 테스트\n",
    "    dummy = torch.randint(0, config.vocab_size, (1, 5))\n",
    "    out = model(dummy)\n",
    "    print('out shape:', out.shape)  # (1,5,50272)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b90082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (token_embedding): Embedding(50272, 768)\n",
       "    (position_embedding): Embedding(2048, 768)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): OPTFeedForward(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677f865a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cad875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "y=torch.tensor([[1,2,3,4],[5,6,7,8]]).reshape(4,2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de1d0c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.1766e-01],\n",
      "         [ 2.3710e-01],\n",
      "         [-3.1125e-01],\n",
      "         [-9.7572e-02],\n",
      "         [-1.2121e+00],\n",
      "         [ 6.6000e-01],\n",
      "         [ 5.5576e-01],\n",
      "         [-7.6201e-01],\n",
      "         [-1.7650e+00],\n",
      "         [-3.1692e-01],\n",
      "         [-1.0299e-01],\n",
      "         [-1.0987e+00],\n",
      "         [ 1.8297e-01],\n",
      "         [ 4.2828e-01],\n",
      "         [ 1.5890e+00],\n",
      "         [-1.2110e+00],\n",
      "         [ 6.7471e-01],\n",
      "         [ 2.3961e-01],\n",
      "         [-2.0213e+00],\n",
      "         [ 7.1468e-01],\n",
      "         [ 2.2473e+00],\n",
      "         [-1.2804e-01],\n",
      "         [-3.4481e-01],\n",
      "         [ 1.1961e+00],\n",
      "         [-5.5442e-01],\n",
      "         [ 1.0019e+00],\n",
      "         [-1.5590e+00],\n",
      "         [-6.3782e-01],\n",
      "         [ 1.2268e+00],\n",
      "         [ 1.1523e+00],\n",
      "         [ 2.0763e-01],\n",
      "         [-2.5591e-01],\n",
      "         [-7.3185e-03],\n",
      "         [ 1.2510e+00],\n",
      "         [ 1.3957e+00],\n",
      "         [-9.8795e-01],\n",
      "         [ 6.8271e-01],\n",
      "         [-3.5732e-01],\n",
      "         [ 2.6001e-02],\n",
      "         [ 5.1747e-01],\n",
      "         [ 1.4850e+00],\n",
      "         [ 1.4697e+00],\n",
      "         [ 4.5662e-01],\n",
      "         [ 1.6208e+00],\n",
      "         [ 2.6890e-01],\n",
      "         [ 2.5941e+00],\n",
      "         [-9.2198e-01],\n",
      "         [-1.8653e+00],\n",
      "         [ 9.3961e-01],\n",
      "         [ 1.3176e+00],\n",
      "         [-2.9173e-01],\n",
      "         [ 9.9864e-01],\n",
      "         [ 1.9237e-02],\n",
      "         [ 1.2739e-01],\n",
      "         [ 8.1461e-01],\n",
      "         [ 4.4827e-01],\n",
      "         [ 1.2886e+00],\n",
      "         [-6.8700e-02],\n",
      "         [-2.1862e-01],\n",
      "         [-5.3794e-02],\n",
      "         [ 4.9263e-01],\n",
      "         [-9.0186e-01],\n",
      "         [ 1.6364e+00],\n",
      "         [-3.8372e-01],\n",
      "         [-7.3250e-01],\n",
      "         [ 1.8342e+00],\n",
      "         [ 3.0645e-01],\n",
      "         [-1.9306e-01],\n",
      "         [-2.2963e+00],\n",
      "         [-1.0478e+00],\n",
      "         [-5.0475e-01],\n",
      "         [ 3.5941e-01],\n",
      "         [ 1.5659e+00],\n",
      "         [ 1.4320e+00],\n",
      "         [-4.8692e-02],\n",
      "         [-1.0094e+00],\n",
      "         [ 3.8760e-01],\n",
      "         [ 1.8735e+00],\n",
      "         [ 5.9930e-01],\n",
      "         [ 3.2439e-01],\n",
      "         [ 6.6622e-01],\n",
      "         [ 8.5903e-01],\n",
      "         [ 7.2715e-01],\n",
      "         [-8.2831e-01],\n",
      "         [-1.2791e+00],\n",
      "         [ 3.9093e-01],\n",
      "         [-9.6478e-01],\n",
      "         [-1.9016e-01],\n",
      "         [ 1.6901e+00],\n",
      "         [ 1.4240e+00],\n",
      "         [ 1.5820e-01],\n",
      "         [ 7.5008e-02],\n",
      "         [-6.5519e-01],\n",
      "         [ 7.5583e-02],\n",
      "         [ 4.5105e-02],\n",
      "         [-9.8854e-01],\n",
      "         [ 5.3373e-01],\n",
      "         [-9.4834e-01],\n",
      "         [ 3.5767e-01],\n",
      "         [-4.8521e-01],\n",
      "         [-5.9966e-01],\n",
      "         [-2.1610e-01],\n",
      "         [-1.9679e+00],\n",
      "         [ 2.8505e-01],\n",
      "         [ 5.4151e-01],\n",
      "         [ 1.0374e+00],\n",
      "         [-1.3525e+00],\n",
      "         [-4.0089e-01],\n",
      "         [-1.1869e-01],\n",
      "         [-7.0573e-01],\n",
      "         [-2.0348e-02],\n",
      "         [-2.6136e-01],\n",
      "         [-1.3733e+00],\n",
      "         [ 5.9299e-01],\n",
      "         [-1.1104e+00],\n",
      "         [-1.0099e+00],\n",
      "         [ 8.7220e-01],\n",
      "         [-4.3448e-01],\n",
      "         [-1.9991e+00],\n",
      "         [ 1.0908e+00],\n",
      "         [ 1.0230e+00],\n",
      "         [-2.9149e-01],\n",
      "         [ 1.5542e+00],\n",
      "         [ 8.9413e-01],\n",
      "         [ 6.5597e-02],\n",
      "         [-5.2732e-01],\n",
      "         [-2.9693e-01],\n",
      "         [-2.1685e+00],\n",
      "         [-5.0335e-01],\n",
      "         [ 8.0674e-02],\n",
      "         [-1.0993e+00],\n",
      "         [ 5.9750e-01],\n",
      "         [-1.3835e-01],\n",
      "         [ 1.2871e+00],\n",
      "         [ 9.1500e-02],\n",
      "         [ 1.7741e+00],\n",
      "         [-4.6049e-01],\n",
      "         [-3.1291e-01],\n",
      "         [-1.3675e+00],\n",
      "         [-5.0981e-01],\n",
      "         [-1.0696e+00],\n",
      "         [ 4.2738e-01],\n",
      "         [-3.4336e-01],\n",
      "         [ 2.0587e-01],\n",
      "         [ 6.6316e-01],\n",
      "         [ 9.4957e-01],\n",
      "         [ 3.9282e-01],\n",
      "         [ 1.4655e+00],\n",
      "         [-8.7310e-01],\n",
      "         [ 1.3920e-01],\n",
      "         [-4.9789e-02],\n",
      "         [-9.2506e-01],\n",
      "         [ 8.8657e-01],\n",
      "         [ 1.6105e+00],\n",
      "         [-2.7413e-02],\n",
      "         [ 8.7971e-01],\n",
      "         [ 3.4404e-01],\n",
      "         [-8.5386e-01],\n",
      "         [ 4.7099e-01],\n",
      "         [ 2.0790e-01],\n",
      "         [-3.1831e-01],\n",
      "         [-4.4050e-01],\n",
      "         [-5.9552e-01],\n",
      "         [ 1.2141e+00],\n",
      "         [-8.6522e-01],\n",
      "         [-1.5357e-01],\n",
      "         [-2.4578e+00],\n",
      "         [-4.2471e-01],\n",
      "         [-5.5275e-01],\n",
      "         [ 3.3855e-01],\n",
      "         [-1.4329e+00],\n",
      "         [-1.1464e+00],\n",
      "         [-1.8571e+00],\n",
      "         [ 1.3651e+00],\n",
      "         [-2.3867e-01],\n",
      "         [ 1.2607e+00],\n",
      "         [ 4.5373e-01],\n",
      "         [-3.7267e-01],\n",
      "         [-1.8592e+00],\n",
      "         [ 1.4892e+00],\n",
      "         [ 1.5899e+00],\n",
      "         [-1.3010e+00],\n",
      "         [ 4.8724e-02],\n",
      "         [-9.2831e-01],\n",
      "         [ 1.2927e+00],\n",
      "         [ 1.2217e-01],\n",
      "         [-1.0795e+00],\n",
      "         [-5.9564e-01],\n",
      "         [ 6.5422e-02],\n",
      "         [ 6.5460e-02],\n",
      "         [-1.5259e+00],\n",
      "         [-1.5795e+00],\n",
      "         [-2.0194e+00],\n",
      "         [-6.8133e-01],\n",
      "         [ 5.0501e-01],\n",
      "         [ 3.5410e+00],\n",
      "         [ 5.7308e-01],\n",
      "         [ 1.4022e+00],\n",
      "         [ 1.6709e-01],\n",
      "         [ 2.5705e+00],\n",
      "         [-4.7993e-01],\n",
      "         [ 6.0277e-01],\n",
      "         [-5.2697e-01],\n",
      "         [ 1.2107e+00],\n",
      "         [ 8.3511e-02],\n",
      "         [ 6.7284e-01],\n",
      "         [-1.1494e+00],\n",
      "         [-1.4334e+00],\n",
      "         [-2.3012e-01],\n",
      "         [-3.3813e-01],\n",
      "         [-1.4199e-01],\n",
      "         [-2.8015e-01],\n",
      "         [-1.2166e+00],\n",
      "         [ 1.2570e+00],\n",
      "         [ 3.1887e-01],\n",
      "         [-8.2717e-01],\n",
      "         [-1.4782e+00],\n",
      "         [ 1.7738e+00],\n",
      "         [ 1.0577e+00],\n",
      "         [-4.4382e-01],\n",
      "         [ 1.3721e+00],\n",
      "         [ 8.7938e-01],\n",
      "         [ 2.3488e-02],\n",
      "         [ 6.7689e-01],\n",
      "         [ 6.1502e-02],\n",
      "         [ 5.6863e-01],\n",
      "         [-6.4296e-02],\n",
      "         [-2.0311e-01],\n",
      "         [ 6.9083e-01],\n",
      "         [-9.4483e-01],\n",
      "         [ 1.3855e-01],\n",
      "         [-2.3860e-02],\n",
      "         [ 5.7969e-01],\n",
      "         [ 5.6367e-01],\n",
      "         [-1.8162e-02],\n",
      "         [ 3.5502e-01],\n",
      "         [ 6.1119e-01],\n",
      "         [ 1.2203e-01],\n",
      "         [-1.8971e-01],\n",
      "         [ 1.0529e-01],\n",
      "         [ 7.8515e-01],\n",
      "         [ 6.2126e-01],\n",
      "         [ 4.6378e-01],\n",
      "         [-4.4059e-01],\n",
      "         [ 1.6032e+00],\n",
      "         [ 1.2432e+00],\n",
      "         [-7.3712e-01],\n",
      "         [ 1.4212e+00],\n",
      "         [-5.3180e-01],\n",
      "         [ 7.1665e-02],\n",
      "         [ 4.9351e-01],\n",
      "         [-1.6486e+00],\n",
      "         [-4.1377e-01],\n",
      "         [ 1.1283e-01],\n",
      "         [-8.9685e-01],\n",
      "         [-3.1279e-01],\n",
      "         [ 4.2931e-01],\n",
      "         [-2.1303e+00],\n",
      "         [-2.2843e-01],\n",
      "         [-1.4806e+00],\n",
      "         [ 6.0463e-01],\n",
      "         [ 4.0687e-01],\n",
      "         [ 6.6751e-01],\n",
      "         [-1.0533e+00],\n",
      "         [ 1.3720e+00],\n",
      "         [ 6.9145e-01],\n",
      "         [-1.3437e-01],\n",
      "         [ 1.0192e+00],\n",
      "         [ 8.6582e-01],\n",
      "         [-2.6257e-01],\n",
      "         [-1.8212e+00],\n",
      "         [ 4.7404e-01],\n",
      "         [-1.8924e+00],\n",
      "         [-2.8562e-01],\n",
      "         [-1.2068e+00],\n",
      "         [ 6.9525e-01],\n",
      "         [-1.1513e+00],\n",
      "         [ 7.0083e-01],\n",
      "         [-1.1599e+00],\n",
      "         [-2.6889e-01],\n",
      "         [-1.1982e+00],\n",
      "         [-7.7971e-01],\n",
      "         [ 2.2766e-02],\n",
      "         [ 7.1609e-02],\n",
      "         [ 2.2051e-02],\n",
      "         [ 1.8000e+00],\n",
      "         [-1.4200e+00],\n",
      "         [ 1.0558e+00],\n",
      "         [ 1.0034e+00],\n",
      "         [ 8.7111e-01],\n",
      "         [-4.4739e-01],\n",
      "         [ 2.2149e-01],\n",
      "         [-3.7365e-01],\n",
      "         [-2.6465e-01],\n",
      "         [-1.0918e+00],\n",
      "         [-2.5116e+00],\n",
      "         [ 2.0993e+00],\n",
      "         [-1.3791e-01],\n",
      "         [ 3.5152e-01],\n",
      "         [-1.4282e+00],\n",
      "         [ 4.3974e-01],\n",
      "         [ 6.1993e-01],\n",
      "         [ 1.8692e+00],\n",
      "         [-6.9934e-01],\n",
      "         [-7.7610e-02],\n",
      "         [-5.0912e-01],\n",
      "         [ 5.2160e-01],\n",
      "         [-1.5601e+00],\n",
      "         [ 9.4653e-01],\n",
      "         [-1.7885e-01],\n",
      "         [-8.8782e-01],\n",
      "         [ 3.6680e-01],\n",
      "         [-1.5424e-02],\n",
      "         [-1.0714e+00],\n",
      "         [-3.2534e-01],\n",
      "         [-1.9738e-02],\n",
      "         [-5.7951e-02],\n",
      "         [ 3.1161e-02],\n",
      "         [-3.9082e-01],\n",
      "         [ 7.6919e-01],\n",
      "         [-8.0281e-01],\n",
      "         [-1.4371e+00],\n",
      "         [-6.6155e-01],\n",
      "         [-1.4238e+00],\n",
      "         [-1.1572e+00],\n",
      "         [ 3.7273e-01],\n",
      "         [-1.7845e-01],\n",
      "         [ 2.3771e-01],\n",
      "         [ 2.8593e-02],\n",
      "         [ 6.8020e-01],\n",
      "         [-1.7918e-01],\n",
      "         [-3.6391e-01],\n",
      "         [-1.7305e-01],\n",
      "         [ 7.4806e-01],\n",
      "         [ 3.8410e-01],\n",
      "         [ 1.3831e+00],\n",
      "         [-4.8212e-01],\n",
      "         [ 5.3299e-01],\n",
      "         [-1.9170e-01],\n",
      "         [-6.4453e-01],\n",
      "         [-1.3172e+00],\n",
      "         [-1.3761e+00],\n",
      "         [-8.8261e-01],\n",
      "         [-1.1408e+00],\n",
      "         [ 1.7924e-01],\n",
      "         [ 2.5440e-01],\n",
      "         [ 9.2394e-01],\n",
      "         [-9.7521e-01],\n",
      "         [ 1.4147e-01],\n",
      "         [ 7.8753e-01],\n",
      "         [ 8.5089e-02],\n",
      "         [ 6.1793e-01],\n",
      "         [-1.3630e+00],\n",
      "         [-1.0241e+00],\n",
      "         [-1.2005e+00],\n",
      "         [-9.3519e-01],\n",
      "         [ 1.0022e+00],\n",
      "         [ 3.9558e-02],\n",
      "         [ 6.3847e-01],\n",
      "         [-4.2077e-01],\n",
      "         [ 1.0051e+00],\n",
      "         [ 2.3035e+00],\n",
      "         [ 7.6488e-01],\n",
      "         [ 2.1970e-01],\n",
      "         [-5.3234e-01],\n",
      "         [-1.9907e-01],\n",
      "         [-1.6707e-01],\n",
      "         [-8.0745e-01],\n",
      "         [ 6.9515e-01],\n",
      "         [-7.5145e-01],\n",
      "         [ 9.8715e-01],\n",
      "         [-4.6053e-01],\n",
      "         [-3.9415e-01],\n",
      "         [ 1.7054e+00],\n",
      "         [ 5.8765e-01],\n",
      "         [ 1.4227e+00],\n",
      "         [-1.1870e+00],\n",
      "         [-3.1827e-01],\n",
      "         [ 1.1917e+00],\n",
      "         [ 6.9365e-01],\n",
      "         [-1.7297e+00],\n",
      "         [ 1.0824e+00],\n",
      "         [ 1.2308e+00],\n",
      "         [ 1.0171e-01],\n",
      "         [-3.3636e-01],\n",
      "         [ 2.9650e-02],\n",
      "         [-4.5489e-01],\n",
      "         [ 1.0960e+00],\n",
      "         [-5.7147e-01],\n",
      "         [-3.0664e-01],\n",
      "         [ 1.8649e+00],\n",
      "         [-1.2235e+00],\n",
      "         [ 4.5720e-01],\n",
      "         [ 7.5145e-01],\n",
      "         [-2.0209e+00],\n",
      "         [-4.8401e-01],\n",
      "         [ 8.6185e-01],\n",
      "         [ 2.7538e-01],\n",
      "         [ 2.9289e-02],\n",
      "         [-1.0637e+00],\n",
      "         [-1.2249e+00],\n",
      "         [ 1.0335e+00],\n",
      "         [-4.0909e-01],\n",
      "         [ 3.9721e-01],\n",
      "         [-1.1783e+00],\n",
      "         [ 5.4816e-01],\n",
      "         [-1.7245e+00],\n",
      "         [-1.4859e-01],\n",
      "         [ 1.1346e+00],\n",
      "         [-4.6047e-01],\n",
      "         [ 3.9154e-02],\n",
      "         [ 6.1172e-01],\n",
      "         [ 1.7535e+00],\n",
      "         [-8.2667e-01],\n",
      "         [-2.0524e+00],\n",
      "         [ 1.8394e+00],\n",
      "         [ 4.9999e-02],\n",
      "         [-1.1891e+00],\n",
      "         [-2.9205e-01],\n",
      "         [-2.7030e-01],\n",
      "         [ 1.0211e+00],\n",
      "         [ 1.6318e+00],\n",
      "         [-4.3212e-01],\n",
      "         [ 6.2385e-01],\n",
      "         [-2.5928e-01],\n",
      "         [-4.1086e-01],\n",
      "         [ 6.5592e-01],\n",
      "         [-9.4407e-01],\n",
      "         [-7.0545e-01],\n",
      "         [ 2.2228e-01],\n",
      "         [-1.0502e+00],\n",
      "         [-2.8451e-01],\n",
      "         [ 2.0108e-01],\n",
      "         [ 7.3626e-01],\n",
      "         [-4.5114e-01],\n",
      "         [-5.6662e-01],\n",
      "         [ 1.9807e-01],\n",
      "         [-2.0866e-01],\n",
      "         [-6.8851e-01],\n",
      "         [-9.9294e-01],\n",
      "         [-1.9758e+00],\n",
      "         [ 1.1892e+00],\n",
      "         [ 3.9644e-01],\n",
      "         [ 8.3988e-01],\n",
      "         [ 7.5788e-02],\n",
      "         [-1.0991e+00],\n",
      "         [-6.4726e-01],\n",
      "         [-3.4637e-01],\n",
      "         [-8.8813e-02],\n",
      "         [-5.3752e-01],\n",
      "         [ 3.5577e-01],\n",
      "         [ 1.7451e-01],\n",
      "         [ 1.7730e-01],\n",
      "         [-9.2453e-01],\n",
      "         [ 1.6854e-01],\n",
      "         [ 8.5342e-01],\n",
      "         [ 9.8173e-01],\n",
      "         [-1.3620e+00],\n",
      "         [ 2.0727e-01],\n",
      "         [-2.4330e+00],\n",
      "         [ 1.6700e+00],\n",
      "         [-1.3204e+00],\n",
      "         [ 7.1126e-01],\n",
      "         [ 1.5403e-01],\n",
      "         [ 5.3472e-01],\n",
      "         [ 5.2364e-01],\n",
      "         [ 8.4211e-01],\n",
      "         [-2.2185e+00],\n",
      "         [ 2.0274e+00],\n",
      "         [-6.1257e-01],\n",
      "         [-1.9776e-01],\n",
      "         [ 6.4104e-01],\n",
      "         [ 6.9940e-02],\n",
      "         [ 9.9524e-01],\n",
      "         [-3.0869e-01],\n",
      "         [-9.9606e-01],\n",
      "         [ 1.6806e-01],\n",
      "         [-9.0825e-01],\n",
      "         [ 1.1187e-01],\n",
      "         [-7.2016e-01],\n",
      "         [-1.5158e+00],\n",
      "         [-3.6684e-01],\n",
      "         [ 1.5442e+00],\n",
      "         [ 1.0183e+00],\n",
      "         [-4.0548e-01],\n",
      "         [ 8.2578e-01],\n",
      "         [-5.8883e-01],\n",
      "         [ 2.2343e-01],\n",
      "         [-1.3815e+00],\n",
      "         [-1.2497e+00],\n",
      "         [-1.1021e+00],\n",
      "         [ 4.3650e-01],\n",
      "         [ 7.1669e-01],\n",
      "         [-9.8919e-01],\n",
      "         [-2.2202e-01],\n",
      "         [ 1.0936e-01],\n",
      "         [ 1.7213e+00],\n",
      "         [ 7.2661e-02],\n",
      "         [ 2.2231e+00],\n",
      "         [ 8.1271e-01],\n",
      "         [-2.8013e+00],\n",
      "         [ 7.7121e-01],\n",
      "         [-2.8759e-01],\n",
      "         [-6.4663e-01],\n",
      "         [-4.9031e-01],\n",
      "         [ 2.9139e-01],\n",
      "         [-1.5570e+00],\n",
      "         [-1.7472e+00],\n",
      "         [-1.8018e+00],\n",
      "         [-1.3212e+00],\n",
      "         [ 8.5174e-01],\n",
      "         [ 1.3285e-01],\n",
      "         [-5.1373e-01],\n",
      "         [-7.7561e-01],\n",
      "         [ 5.0987e-01],\n",
      "         [ 3.5347e-01],\n",
      "         [-6.6358e-01],\n",
      "         [ 1.5138e+00],\n",
      "         [ 2.2832e+00],\n",
      "         [-2.4900e-01],\n",
      "         [ 1.0747e+00],\n",
      "         [ 1.5995e-03],\n",
      "         [ 2.2717e+00],\n",
      "         [-3.8707e-01],\n",
      "         [-1.7547e+00],\n",
      "         [-1.2997e+00],\n",
      "         [-6.0646e-01],\n",
      "         [ 3.3928e-02],\n",
      "         [-7.4032e-01],\n",
      "         [ 5.0152e-01],\n",
      "         [-8.1301e-01],\n",
      "         [-1.0748e+00],\n",
      "         [ 2.5607e-01],\n",
      "         [-1.7911e+00],\n",
      "         [ 1.3565e+00],\n",
      "         [ 1.8867e+00],\n",
      "         [-1.2212e+00],\n",
      "         [ 8.7613e-01],\n",
      "         [-1.9316e+00],\n",
      "         [ 2.3488e-02],\n",
      "         [-1.0738e+00],\n",
      "         [-6.1623e-01],\n",
      "         [-3.5155e-01],\n",
      "         [ 5.0459e-01],\n",
      "         [ 1.9033e-01],\n",
      "         [ 7.7887e-03],\n",
      "         [-2.3165e+00],\n",
      "         [-1.2603e+00],\n",
      "         [-1.0207e+00],\n",
      "         [-4.0083e-01],\n",
      "         [-2.5149e+00],\n",
      "         [ 1.5255e+00],\n",
      "         [ 6.7989e-02],\n",
      "         [-3.6705e-01],\n",
      "         [ 1.1989e+00],\n",
      "         [-1.0890e+00],\n",
      "         [ 3.7973e-01],\n",
      "         [ 1.0935e+00],\n",
      "         [-5.4127e-01],\n",
      "         [-5.0141e-01],\n",
      "         [-5.1265e-01],\n",
      "         [ 9.0342e-02],\n",
      "         [-1.6851e+00],\n",
      "         [ 1.1216e+00],\n",
      "         [ 1.6404e+00],\n",
      "         [-1.6503e+00],\n",
      "         [ 2.7195e+00],\n",
      "         [-6.7000e-01],\n",
      "         [ 1.3828e-01],\n",
      "         [ 3.2739e-01],\n",
      "         [-5.8871e-01],\n",
      "         [-4.1483e-01],\n",
      "         [ 1.2860e+00],\n",
      "         [-6.3776e-01],\n",
      "         [-1.3352e-01],\n",
      "         [-1.8374e+00],\n",
      "         [ 7.9479e-01],\n",
      "         [ 3.6026e-02],\n",
      "         [-1.4401e+00],\n",
      "         [ 9.6296e-01],\n",
      "         [-3.8709e-01],\n",
      "         [-1.5859e-01],\n",
      "         [ 3.8539e-01],\n",
      "         [-9.5798e-01],\n",
      "         [ 1.4952e+00],\n",
      "         [ 2.3918e-02],\n",
      "         [-2.9517e-01],\n",
      "         [ 6.2766e-01],\n",
      "         [-8.7339e-01],\n",
      "         [ 1.0379e+00],\n",
      "         [-1.0507e+00],\n",
      "         [-6.5154e-01],\n",
      "         [ 5.0510e-01],\n",
      "         [-3.3131e-01],\n",
      "         [ 1.3648e-01],\n",
      "         [ 1.5274e+00],\n",
      "         [ 8.1596e-01],\n",
      "         [ 4.1830e-01],\n",
      "         [-7.8762e-01],\n",
      "         [-1.2945e+00],\n",
      "         [ 2.4888e-01],\n",
      "         [ 1.3890e+00],\n",
      "         [-6.7076e-01],\n",
      "         [ 4.9192e-01],\n",
      "         [-8.8622e-01],\n",
      "         [-7.9308e-01],\n",
      "         [-3.8563e-01],\n",
      "         [-5.2374e-01],\n",
      "         [-1.5873e-01],\n",
      "         [ 5.9554e-01],\n",
      "         [-3.0470e-01],\n",
      "         [ 9.0047e-02],\n",
      "         [ 2.4342e-02],\n",
      "         [-1.8350e+00],\n",
      "         [ 5.7043e-01],\n",
      "         [ 1.7149e+00],\n",
      "         [-3.9336e-01],\n",
      "         [-5.5455e-01],\n",
      "         [-9.1420e-01],\n",
      "         [ 8.9566e-01],\n",
      "         [ 1.5912e-01],\n",
      "         [-1.1354e+00],\n",
      "         [ 4.7120e-01],\n",
      "         [ 2.2717e+00],\n",
      "         [ 9.9156e-02],\n",
      "         [-5.6989e-01],\n",
      "         [-1.7337e-01],\n",
      "         [-1.0813e+00],\n",
      "         [-5.7612e-01],\n",
      "         [ 5.7009e-02],\n",
      "         [ 5.5633e-01],\n",
      "         [ 7.1051e-01],\n",
      "         [ 8.6983e-01],\n",
      "         [-5.4284e-01],\n",
      "         [ 2.3557e-01],\n",
      "         [ 2.3707e+00],\n",
      "         [-1.0421e+00],\n",
      "         [-4.8781e-01],\n",
      "         [ 8.7089e-01],\n",
      "         [ 6.1745e-01],\n",
      "         [ 9.2338e-01],\n",
      "         [-7.4646e-02],\n",
      "         [-3.6660e-01],\n",
      "         [-1.5299e+00],\n",
      "         [-2.2729e-01],\n",
      "         [ 9.7452e-01],\n",
      "         [-4.9221e-02],\n",
      "         [ 1.5305e-01],\n",
      "         [ 1.9916e-01],\n",
      "         [-2.7942e-01],\n",
      "         [ 1.3007e+00],\n",
      "         [-6.2701e-01],\n",
      "         [-4.9349e-01],\n",
      "         [-9.4651e-02],\n",
      "         [ 2.0366e+00],\n",
      "         [ 2.5036e-01],\n",
      "         [-5.3496e-01],\n",
      "         [-2.2715e-01],\n",
      "         [ 9.3963e-01],\n",
      "         [ 3.2797e-01],\n",
      "         [ 3.6017e-01],\n",
      "         [ 2.8671e-01],\n",
      "         [-1.5273e+00],\n",
      "         [ 1.9625e+00],\n",
      "         [ 2.5086e+00],\n",
      "         [ 1.9299e-01],\n",
      "         [ 5.7171e-01],\n",
      "         [-1.5308e+00],\n",
      "         [-1.6255e+00],\n",
      "         [ 1.0923e+00],\n",
      "         [ 2.9867e-01],\n",
      "         [ 5.0483e-01],\n",
      "         [ 8.5769e-01],\n",
      "         [-8.2001e-01],\n",
      "         [-5.8353e-01],\n",
      "         [ 4.8156e-03],\n",
      "         [-7.5041e-01],\n",
      "         [-1.4948e+00],\n",
      "         [-6.6687e-01],\n",
      "         [ 4.8127e-01],\n",
      "         [-8.2804e-01],\n",
      "         [ 2.8532e-02],\n",
      "         [-1.9732e-01],\n",
      "         [ 9.1767e-01],\n",
      "         [ 9.0660e-01],\n",
      "         [ 2.0030e+00],\n",
      "         [ 2.4989e-01],\n",
      "         [-1.3783e+00],\n",
      "         [-1.7329e+00],\n",
      "         [-2.2558e+00],\n",
      "         [ 1.7077e-01],\n",
      "         [ 1.7658e+00],\n",
      "         [ 1.2557e+00],\n",
      "         [-1.4919e+00],\n",
      "         [ 1.2566e+00],\n",
      "         [-1.6740e+00],\n",
      "         [-3.4780e-01],\n",
      "         [-9.4309e-01],\n",
      "         [-5.4523e-01],\n",
      "         [ 1.4183e+00],\n",
      "         [ 4.9449e-01],\n",
      "         [ 1.0077e+00],\n",
      "         [ 2.5631e+00],\n",
      "         [-1.0746e+00],\n",
      "         [-2.1765e-01],\n",
      "         [-7.6085e-01],\n",
      "         [ 2.3016e-01],\n",
      "         [-4.2165e-01],\n",
      "         [-7.8401e-01],\n",
      "         [-2.2971e+00],\n",
      "         [-9.3417e-01],\n",
      "         [ 2.6599e-01],\n",
      "         [-1.3914e+00],\n",
      "         [-4.1584e-01],\n",
      "         [ 8.1025e-01],\n",
      "         [-1.9761e-01],\n",
      "         [ 3.5320e-01],\n",
      "         [ 1.3814e+00],\n",
      "         [ 9.0917e-02],\n",
      "         [-2.7577e-01],\n",
      "         [-3.7462e-01],\n",
      "         [-5.7546e-01],\n",
      "         [ 8.8694e-01],\n",
      "         [ 1.8014e-01],\n",
      "         [-2.9896e+00],\n",
      "         [-9.3210e-01],\n",
      "         [-1.4353e+00],\n",
      "         [-6.7449e-02],\n",
      "         [-1.1471e+00],\n",
      "         [-6.2356e-02],\n",
      "         [ 1.2166e+00],\n",
      "         [ 4.2884e-02],\n",
      "         [ 9.2061e-01],\n",
      "         [-2.1111e-01],\n",
      "         [ 1.1639e+00],\n",
      "         [-6.3145e-01],\n",
      "         [-1.9293e-01],\n",
      "         [ 3.4150e-01],\n",
      "         [-4.6616e-02],\n",
      "         [ 7.0739e-01],\n",
      "         [-2.0087e+00],\n",
      "         [ 2.8306e-01],\n",
      "         [ 6.6146e-01],\n",
      "         [ 2.4092e-01],\n",
      "         [-1.3657e+00],\n",
      "         [-7.0843e-01],\n",
      "         [ 6.2013e-01],\n",
      "         [ 1.7255e+00],\n",
      "         [ 4.7868e-01],\n",
      "         [ 1.8916e-01],\n",
      "         [-7.3705e-01],\n",
      "         [ 1.0002e+00],\n",
      "         [-8.2681e-01],\n",
      "         [ 5.6950e-01],\n",
      "         [-1.8697e-01],\n",
      "         [ 5.3314e-01],\n",
      "         [ 4.8594e-01],\n",
      "         [-1.2134e+00],\n",
      "         [ 1.8343e+00],\n",
      "         [ 7.9036e-01],\n",
      "         [ 3.5707e-01],\n",
      "         [ 1.2249e+00],\n",
      "         [-8.0554e-01],\n",
      "         [-6.5432e-01],\n",
      "         [ 1.3332e-01],\n",
      "         [-1.6030e+00],\n",
      "         [-5.8357e-02],\n",
      "         [ 7.5342e-01]]])\n",
      "tensor([[ 5.1766e-01,  2.3710e-01, -3.1125e-01, -9.7572e-02, -1.2121e+00,\n",
      "          6.6000e-01,  5.5576e-01, -7.6201e-01, -1.7650e+00, -3.1692e-01,\n",
      "         -1.0299e-01, -1.0987e+00,  1.8297e-01,  4.2828e-01,  1.5890e+00,\n",
      "         -1.2110e+00,  6.7471e-01,  2.3961e-01, -2.0213e+00,  7.1468e-01,\n",
      "          2.2473e+00, -1.2804e-01, -3.4481e-01,  1.1961e+00, -5.5442e-01,\n",
      "          1.0019e+00, -1.5590e+00, -6.3782e-01,  1.2268e+00,  1.1523e+00,\n",
      "          2.0763e-01, -2.5591e-01, -7.3185e-03,  1.2510e+00,  1.3957e+00,\n",
      "         -9.8795e-01,  6.8271e-01, -3.5732e-01,  2.6001e-02,  5.1747e-01,\n",
      "          1.4850e+00,  1.4697e+00,  4.5662e-01,  1.6208e+00,  2.6890e-01,\n",
      "          2.5941e+00, -9.2198e-01, -1.8653e+00,  9.3961e-01,  1.3176e+00,\n",
      "         -2.9173e-01,  9.9864e-01,  1.9237e-02,  1.2739e-01,  8.1461e-01,\n",
      "          4.4827e-01,  1.2886e+00, -6.8700e-02, -2.1862e-01, -5.3794e-02,\n",
      "          4.9263e-01, -9.0186e-01,  1.6364e+00, -3.8372e-01, -7.3250e-01,\n",
      "          1.8342e+00,  3.0645e-01, -1.9306e-01, -2.2963e+00, -1.0478e+00,\n",
      "         -5.0475e-01,  3.5941e-01,  1.5659e+00,  1.4320e+00, -4.8692e-02,\n",
      "         -1.0094e+00,  3.8760e-01,  1.8735e+00,  5.9930e-01,  3.2439e-01,\n",
      "          6.6622e-01,  8.5903e-01,  7.2715e-01, -8.2831e-01, -1.2791e+00,\n",
      "          3.9093e-01, -9.6478e-01, -1.9016e-01,  1.6901e+00,  1.4240e+00,\n",
      "          1.5820e-01,  7.5008e-02, -6.5519e-01,  7.5583e-02,  4.5105e-02,\n",
      "         -9.8854e-01,  5.3373e-01, -9.4834e-01,  3.5767e-01, -4.8521e-01,\n",
      "         -5.9966e-01, -2.1610e-01, -1.9679e+00,  2.8505e-01,  5.4151e-01,\n",
      "          1.0374e+00, -1.3525e+00, -4.0089e-01, -1.1869e-01, -7.0573e-01,\n",
      "         -2.0348e-02, -2.6136e-01, -1.3733e+00,  5.9299e-01, -1.1104e+00,\n",
      "         -1.0099e+00,  8.7220e-01, -4.3448e-01, -1.9991e+00,  1.0908e+00,\n",
      "          1.0230e+00, -2.9149e-01,  1.5542e+00,  8.9413e-01,  6.5597e-02,\n",
      "         -5.2732e-01, -2.9693e-01, -2.1685e+00, -5.0335e-01,  8.0674e-02,\n",
      "         -1.0993e+00,  5.9750e-01, -1.3835e-01,  1.2871e+00,  9.1500e-02,\n",
      "          1.7741e+00, -4.6049e-01, -3.1291e-01, -1.3675e+00, -5.0981e-01,\n",
      "         -1.0696e+00,  4.2738e-01, -3.4336e-01,  2.0587e-01,  6.6316e-01,\n",
      "          9.4957e-01,  3.9282e-01,  1.4655e+00, -8.7310e-01,  1.3920e-01,\n",
      "         -4.9789e-02, -9.2506e-01,  8.8657e-01,  1.6105e+00, -2.7413e-02,\n",
      "          8.7971e-01,  3.4404e-01, -8.5386e-01,  4.7099e-01,  2.0790e-01,\n",
      "         -3.1831e-01, -4.4050e-01, -5.9552e-01,  1.2141e+00, -8.6522e-01,\n",
      "         -1.5357e-01, -2.4578e+00, -4.2471e-01, -5.5275e-01,  3.3855e-01,\n",
      "         -1.4329e+00, -1.1464e+00, -1.8571e+00,  1.3651e+00, -2.3867e-01,\n",
      "          1.2607e+00,  4.5373e-01, -3.7267e-01, -1.8592e+00,  1.4892e+00,\n",
      "          1.5899e+00, -1.3010e+00,  4.8724e-02, -9.2831e-01,  1.2927e+00,\n",
      "          1.2217e-01, -1.0795e+00, -5.9564e-01,  6.5422e-02,  6.5460e-02,\n",
      "         -1.5259e+00, -1.5795e+00, -2.0194e+00, -6.8133e-01,  5.0501e-01,\n",
      "          3.5410e+00,  5.7308e-01,  1.4022e+00,  1.6709e-01,  2.5705e+00,\n",
      "         -4.7993e-01,  6.0277e-01, -5.2697e-01,  1.2107e+00,  8.3511e-02,\n",
      "          6.7284e-01, -1.1494e+00, -1.4334e+00, -2.3012e-01, -3.3813e-01,\n",
      "         -1.4199e-01, -2.8015e-01, -1.2166e+00,  1.2570e+00,  3.1887e-01,\n",
      "         -8.2717e-01, -1.4782e+00,  1.7738e+00,  1.0577e+00, -4.4382e-01,\n",
      "          1.3721e+00,  8.7938e-01,  2.3488e-02,  6.7689e-01,  6.1502e-02,\n",
      "          5.6863e-01, -6.4296e-02, -2.0311e-01,  6.9083e-01, -9.4483e-01,\n",
      "          1.3855e-01, -2.3860e-02,  5.7969e-01,  5.6367e-01, -1.8162e-02,\n",
      "          3.5502e-01,  6.1119e-01,  1.2203e-01, -1.8971e-01,  1.0529e-01,\n",
      "          7.8515e-01,  6.2126e-01,  4.6378e-01, -4.4059e-01,  1.6032e+00,\n",
      "          1.2432e+00, -7.3712e-01,  1.4212e+00, -5.3180e-01,  7.1665e-02,\n",
      "          4.9351e-01, -1.6486e+00, -4.1377e-01,  1.1283e-01, -8.9685e-01,\n",
      "         -3.1279e-01,  4.2931e-01, -2.1303e+00, -2.2843e-01, -1.4806e+00,\n",
      "          6.0463e-01,  4.0687e-01,  6.6751e-01, -1.0533e+00,  1.3720e+00,\n",
      "          6.9145e-01, -1.3437e-01,  1.0192e+00,  8.6582e-01, -2.6257e-01,\n",
      "         -1.8212e+00,  4.7404e-01, -1.8924e+00, -2.8562e-01, -1.2068e+00,\n",
      "          6.9525e-01, -1.1513e+00,  7.0083e-01, -1.1599e+00, -2.6889e-01,\n",
      "         -1.1982e+00, -7.7971e-01,  2.2766e-02,  7.1609e-02,  2.2051e-02,\n",
      "          1.8000e+00, -1.4200e+00,  1.0558e+00,  1.0034e+00,  8.7111e-01,\n",
      "         -4.4739e-01,  2.2149e-01, -3.7365e-01, -2.6465e-01, -1.0918e+00,\n",
      "         -2.5116e+00,  2.0993e+00, -1.3791e-01,  3.5152e-01, -1.4282e+00,\n",
      "          4.3974e-01,  6.1993e-01,  1.8692e+00, -6.9934e-01, -7.7610e-02,\n",
      "         -5.0912e-01,  5.2160e-01, -1.5601e+00,  9.4653e-01, -1.7885e-01,\n",
      "         -8.8782e-01,  3.6680e-01, -1.5424e-02, -1.0714e+00, -3.2534e-01,\n",
      "         -1.9738e-02, -5.7951e-02,  3.1161e-02, -3.9082e-01,  7.6919e-01,\n",
      "         -8.0281e-01, -1.4371e+00, -6.6155e-01, -1.4238e+00, -1.1572e+00,\n",
      "          3.7273e-01, -1.7845e-01,  2.3771e-01,  2.8593e-02,  6.8020e-01,\n",
      "         -1.7918e-01, -3.6391e-01, -1.7305e-01,  7.4806e-01,  3.8410e-01,\n",
      "          1.3831e+00, -4.8212e-01,  5.3299e-01, -1.9170e-01, -6.4453e-01,\n",
      "         -1.3172e+00, -1.3761e+00, -8.8261e-01, -1.1408e+00,  1.7924e-01,\n",
      "          2.5440e-01,  9.2394e-01, -9.7521e-01,  1.4147e-01,  7.8753e-01,\n",
      "          8.5089e-02,  6.1793e-01, -1.3630e+00, -1.0241e+00, -1.2005e+00,\n",
      "         -9.3519e-01,  1.0022e+00,  3.9558e-02,  6.3847e-01, -4.2077e-01,\n",
      "          1.0051e+00,  2.3035e+00,  7.6488e-01,  2.1970e-01, -5.3234e-01,\n",
      "         -1.9907e-01, -1.6707e-01, -8.0745e-01,  6.9515e-01, -7.5145e-01,\n",
      "          9.8715e-01, -4.6053e-01, -3.9415e-01,  1.7054e+00,  5.8765e-01,\n",
      "          1.4227e+00, -1.1870e+00, -3.1827e-01,  1.1917e+00,  6.9365e-01,\n",
      "         -1.7297e+00,  1.0824e+00,  1.2308e+00,  1.0171e-01, -3.3636e-01,\n",
      "          2.9650e-02, -4.5489e-01,  1.0960e+00, -5.7147e-01, -3.0664e-01,\n",
      "          1.8649e+00, -1.2235e+00,  4.5720e-01,  7.5145e-01, -2.0209e+00,\n",
      "         -4.8401e-01,  8.6185e-01,  2.7538e-01,  2.9289e-02, -1.0637e+00,\n",
      "         -1.2249e+00,  1.0335e+00, -4.0909e-01,  3.9721e-01, -1.1783e+00,\n",
      "          5.4816e-01, -1.7245e+00, -1.4859e-01,  1.1346e+00, -4.6047e-01,\n",
      "          3.9154e-02,  6.1172e-01,  1.7535e+00, -8.2667e-01, -2.0524e+00,\n",
      "          1.8394e+00,  4.9999e-02, -1.1891e+00, -2.9205e-01, -2.7030e-01,\n",
      "          1.0211e+00,  1.6318e+00, -4.3212e-01,  6.2385e-01, -2.5928e-01,\n",
      "         -4.1086e-01,  6.5592e-01, -9.4407e-01, -7.0545e-01,  2.2228e-01,\n",
      "         -1.0502e+00, -2.8451e-01,  2.0108e-01,  7.3626e-01, -4.5114e-01,\n",
      "         -5.6662e-01,  1.9807e-01, -2.0866e-01, -6.8851e-01, -9.9294e-01,\n",
      "         -1.9758e+00,  1.1892e+00,  3.9644e-01,  8.3988e-01,  7.5788e-02,\n",
      "         -1.0991e+00, -6.4726e-01, -3.4637e-01, -8.8813e-02, -5.3752e-01,\n",
      "          3.5577e-01,  1.7451e-01,  1.7730e-01, -9.2453e-01,  1.6854e-01,\n",
      "          8.5342e-01,  9.8173e-01, -1.3620e+00,  2.0727e-01, -2.4330e+00,\n",
      "          1.6700e+00, -1.3204e+00,  7.1126e-01,  1.5403e-01,  5.3472e-01,\n",
      "          5.2364e-01,  8.4211e-01, -2.2185e+00,  2.0274e+00, -6.1257e-01,\n",
      "         -1.9776e-01,  6.4104e-01,  6.9940e-02,  9.9524e-01, -3.0869e-01,\n",
      "         -9.9606e-01,  1.6806e-01, -9.0825e-01,  1.1187e-01, -7.2016e-01,\n",
      "         -1.5158e+00, -3.6684e-01,  1.5442e+00,  1.0183e+00, -4.0548e-01,\n",
      "          8.2578e-01, -5.8883e-01,  2.2343e-01, -1.3815e+00, -1.2497e+00,\n",
      "         -1.1021e+00,  4.3650e-01,  7.1669e-01, -9.8919e-01, -2.2202e-01,\n",
      "          1.0936e-01,  1.7213e+00,  7.2661e-02,  2.2231e+00,  8.1271e-01,\n",
      "         -2.8013e+00,  7.7121e-01, -2.8759e-01, -6.4663e-01, -4.9031e-01,\n",
      "          2.9139e-01, -1.5570e+00, -1.7472e+00, -1.8018e+00, -1.3212e+00,\n",
      "          8.5174e-01,  1.3285e-01, -5.1373e-01, -7.7561e-01,  5.0987e-01,\n",
      "          3.5347e-01, -6.6358e-01,  1.5138e+00,  2.2832e+00, -2.4900e-01,\n",
      "          1.0747e+00,  1.5995e-03,  2.2717e+00, -3.8707e-01, -1.7547e+00,\n",
      "         -1.2997e+00, -6.0646e-01,  3.3928e-02, -7.4032e-01,  5.0152e-01,\n",
      "         -8.1301e-01, -1.0748e+00,  2.5607e-01, -1.7911e+00,  1.3565e+00,\n",
      "          1.8867e+00, -1.2212e+00,  8.7613e-01, -1.9316e+00,  2.3488e-02,\n",
      "         -1.0738e+00, -6.1623e-01, -3.5155e-01,  5.0459e-01,  1.9033e-01,\n",
      "          7.7887e-03, -2.3165e+00, -1.2603e+00, -1.0207e+00, -4.0083e-01,\n",
      "         -2.5149e+00,  1.5255e+00,  6.7989e-02, -3.6705e-01,  1.1989e+00,\n",
      "         -1.0890e+00,  3.7973e-01,  1.0935e+00, -5.4127e-01, -5.0141e-01,\n",
      "         -5.1265e-01,  9.0342e-02, -1.6851e+00,  1.1216e+00,  1.6404e+00,\n",
      "         -1.6503e+00,  2.7195e+00, -6.7000e-01,  1.3828e-01,  3.2739e-01,\n",
      "         -5.8871e-01, -4.1483e-01,  1.2860e+00, -6.3776e-01, -1.3352e-01,\n",
      "         -1.8374e+00,  7.9479e-01,  3.6026e-02, -1.4401e+00,  9.6296e-01,\n",
      "         -3.8709e-01, -1.5859e-01,  3.8539e-01, -9.5798e-01,  1.4952e+00,\n",
      "          2.3918e-02, -2.9517e-01,  6.2766e-01, -8.7339e-01,  1.0379e+00,\n",
      "         -1.0507e+00, -6.5154e-01,  5.0510e-01, -3.3131e-01,  1.3648e-01,\n",
      "          1.5274e+00,  8.1596e-01,  4.1830e-01, -7.8762e-01, -1.2945e+00,\n",
      "          2.4888e-01,  1.3890e+00, -6.7076e-01,  4.9192e-01, -8.8622e-01,\n",
      "         -7.9308e-01, -3.8563e-01, -5.2374e-01, -1.5873e-01,  5.9554e-01,\n",
      "         -3.0470e-01,  9.0047e-02,  2.4342e-02, -1.8350e+00,  5.7043e-01,\n",
      "          1.7149e+00, -3.9336e-01, -5.5455e-01, -9.1420e-01,  8.9566e-01,\n",
      "          1.5912e-01, -1.1354e+00,  4.7120e-01,  2.2717e+00,  9.9156e-02,\n",
      "         -5.6989e-01, -1.7337e-01, -1.0813e+00, -5.7612e-01,  5.7009e-02,\n",
      "          5.5633e-01,  7.1051e-01,  8.6983e-01, -5.4284e-01,  2.3557e-01,\n",
      "          2.3707e+00, -1.0421e+00, -4.8781e-01,  8.7089e-01,  6.1745e-01,\n",
      "          9.2338e-01, -7.4646e-02, -3.6660e-01, -1.5299e+00, -2.2729e-01,\n",
      "          9.7452e-01, -4.9221e-02,  1.5305e-01,  1.9916e-01, -2.7942e-01,\n",
      "          1.3007e+00, -6.2701e-01, -4.9349e-01, -9.4651e-02,  2.0366e+00,\n",
      "          2.5036e-01, -5.3496e-01, -2.2715e-01,  9.3963e-01,  3.2797e-01,\n",
      "          3.6017e-01,  2.8671e-01, -1.5273e+00,  1.9625e+00,  2.5086e+00,\n",
      "          1.9299e-01,  5.7171e-01, -1.5308e+00, -1.6255e+00,  1.0923e+00,\n",
      "          2.9867e-01,  5.0483e-01,  8.5769e-01, -8.2001e-01, -5.8353e-01,\n",
      "          4.8156e-03, -7.5041e-01, -1.4948e+00, -6.6687e-01,  4.8127e-01,\n",
      "         -8.2804e-01,  2.8532e-02, -1.9732e-01,  9.1767e-01,  9.0660e-01,\n",
      "          2.0030e+00,  2.4989e-01, -1.3783e+00, -1.7329e+00, -2.2558e+00,\n",
      "          1.7077e-01,  1.7658e+00,  1.2557e+00, -1.4919e+00,  1.2566e+00,\n",
      "         -1.6740e+00, -3.4780e-01, -9.4309e-01, -5.4523e-01,  1.4183e+00,\n",
      "          4.9449e-01,  1.0077e+00,  2.5631e+00, -1.0746e+00, -2.1765e-01,\n",
      "         -7.6085e-01,  2.3016e-01, -4.2165e-01, -7.8401e-01, -2.2971e+00,\n",
      "         -9.3417e-01,  2.6599e-01, -1.3914e+00, -4.1584e-01,  8.1025e-01,\n",
      "         -1.9761e-01,  3.5320e-01,  1.3814e+00,  9.0917e-02, -2.7577e-01,\n",
      "         -3.7462e-01, -5.7546e-01,  8.8694e-01,  1.8014e-01, -2.9896e+00,\n",
      "         -9.3210e-01, -1.4353e+00, -6.7449e-02, -1.1471e+00, -6.2356e-02,\n",
      "          1.2166e+00,  4.2884e-02,  9.2061e-01, -2.1111e-01,  1.1639e+00,\n",
      "         -6.3145e-01, -1.9293e-01,  3.4150e-01, -4.6616e-02,  7.0739e-01,\n",
      "         -2.0087e+00,  2.8306e-01,  6.6146e-01,  2.4092e-01, -1.3657e+00,\n",
      "         -7.0843e-01,  6.2013e-01,  1.7255e+00,  4.7868e-01,  1.8916e-01,\n",
      "         -7.3705e-01,  1.0002e+00, -8.2681e-01,  5.6950e-01, -1.8697e-01,\n",
      "          5.3314e-01,  4.8594e-01, -1.2134e+00,  1.8343e+00,  7.9036e-01,\n",
      "          3.5707e-01,  1.2249e+00, -8.0554e-01, -6.5432e-01,  1.3332e-01,\n",
      "         -1.6030e+00, -5.8357e-02,  7.5342e-01]])\n"
     ]
    }
   ],
   "source": [
    "y=torch.randn(1, 768, 1)\n",
    "print(y)\n",
    "y= y.reshape(64, 12).reshape(-1, 768)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75401f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ad933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch=1\n",
    "hidden=768\n",
    "sequence_length=128\n",
    "new_token_length=1\n",
    "num_head=12\n",
    "# init memory\n",
    "act_buffer = torch.zeros(256, 12)    # (your real buffer)\n",
    "k_cache = torch.zeros(64*128, 12)\n",
    "v_cache = torch.zeros(64*128, 12)\n",
    "\n",
    "# init new token\n",
    "new_token = torch.randn(batch, hidden, new_token_length)\n",
    "act_buffer[0:64, :]=new_token.reshape(64, 12)\n",
    "\n",
    "\n",
    "## ===========\n",
    "## Q projection\n",
    "## ============\n",
    "\n",
    "W = model.model.layers[0].self_attn.q_proj.weight    \n",
    "b = model.model.layers[0].self_attn.q_proj.bias      \n",
    "\n",
    "W = W.T   \n",
    "\n",
    "# 3. Define your split parameters\n",
    "num_pe = 96\n",
    "num_acc = int(hidden/num_pe) # 32\n",
    "\n",
    "# 4. Allocate y with the correct output size\n",
    "x = act_buffer[0:64, :].reshape(-1, 768)\n",
    "y = torch.zeros(1, hidden)\n",
    "\n",
    "# 5. Loop over each of the 2304 output rows, dot‐product in 32 pieces\n",
    "for j in range(hidden):\n",
    "    acc = 0.0\n",
    "    for i in range(num_acc):\n",
    "        start = i * num_pe\n",
    "        end   = start + num_pe\n",
    "        # x[:, start:end] is [1×24], W[j, start:end] is [24]\n",
    "        acc += (x[:, start:end] * W[j, start:end]).sum()\n",
    "    y[0, j] = acc + b[j]\n",
    "act_buffer[0:64, :]=y.reshape(64, 12)\n",
    "\n",
    "\n",
    "## =====\n",
    "## K projection \n",
    "## =====\n",
    "\n",
    "W = model.model.layers[0].self_attn.k_proj.weight    \n",
    "b = model.model.layers[0].self_attn.k_proj.bias      \n",
    "\n",
    "W = W.T \n",
    "# 4. Allocate y with the correct output size\n",
    "x = act_buffer[0:64, :].reshape(-1, 768)\n",
    "y = torch.zeros(1, hidden)\n",
    "\n",
    "# 5. Loop over each of the 2304 output rows, dot‐product in 32 pieces\n",
    "for j in range(hidden):\n",
    "    acc = 0.0\n",
    "    for i in range(num_acc):\n",
    "        start = i * num_pe\n",
    "        end   = start + num_pe\n",
    "        # x[:, start:end] is [1×24], W[j, start:end] is [24]\n",
    "        acc += (x[:, start:end] * W[j, start:end]).sum()\n",
    "    y[0, j] = acc + b[j]\n",
    "k_cache[0:64, :]=y.reshape(64, 12)\n",
    "\n",
    "\n",
    "\n",
    "## =====\n",
    "## V projection \n",
    "## =====\n",
    "\n",
    "W = model.model.layers[0].self_attn.v_proj.weight    \n",
    "b = model.model.layers[0].self_attn.v_proj.bias      \n",
    "\n",
    "W = W.T \n",
    "# 4. Allocate y with the correct output size\n",
    "x = act_buffer[0:64, :].reshape(-1, 768)\n",
    "y = torch.zeros(1, hidden)\n",
    "\n",
    "# 5. Loop over each of the 2304 output rows, dot‐product in 32 pieces\n",
    "for j in range(hidden):\n",
    "    acc = 0.0\n",
    "    for i in range(num_acc):\n",
    "        start = i * num_pe\n",
    "        end   = start + num_pe\n",
    "        # x[:, start:end] is [1×24], W[j, start:end] is [24]\n",
    "        acc += (x[:, start:end] * W[j, start:end]).sum()\n",
    "    y[0, j] = acc + b[j]\n",
    "v_cache[0:64, :]=y.reshape(64, 12)\n",
    "\n",
    "## ====\n",
    "## attention score\n",
    "## ====\n",
    "num_token=3\n",
    "y=torch.zeros(num_token, num_head)\n",
    "for j in range(num_token):\n",
    "    for i in range(64):\n",
    "        for k in range(num_head):\n",
    "            y[j:j+1, k:k+1]+=act_buffer[i,k:k+1 ] * k_cache[i+num_token*64, k:k+1]\n",
    "y=F.softmax(y, dim=0)\n",
    "act_buffer[0:num_token, 0:num_head] = y #(num_token, num_head)\n",
    "\n",
    "## === \n",
    "## attention score * V\n",
    "## ===\n",
    "for i in range(num_token):\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4875e526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_cache[64]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

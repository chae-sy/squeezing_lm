{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "# basic configs\n",
        "batch = 1\n",
        "hidden = 768\n",
        "sequence_length = 2048\n",
        "new_token_length = 1\n",
        "num_head = 12\n",
        "head_dim = hidden // num_head  # 64\n",
        "num_layer = 12\n",
        "intermediate_size = 3072\n",
        "\n",
        "device = \"cpu\"\n",
        "dtype = torch.float32\n",
        "\n",
        "# Q/K/V/out proj\n",
        "W_q = torch.randn(hidden, hidden, dtype=dtype, device=device)\n",
        "b_q = torch.randn(hidden, 1,      dtype=dtype, device=device)\n",
        "W_k = torch.randn(hidden, hidden, dtype=dtype, device=device)\n",
        "b_k = torch.randn(hidden, 1,      dtype=dtype, device=device)\n",
        "W_v = torch.randn(hidden, hidden, dtype=dtype, device=device)\n",
        "b_v = torch.randn(hidden, 1,      dtype=dtype, device=device)\n",
        "W_o = torch.randn(hidden, hidden, dtype=dtype, device=device)\n",
        "b_o = torch.randn(hidden, 1,      dtype=dtype, device=device)\n",
        "\n",
        "# MLP (fc1, fc2) + biases\n",
        "W_fc1 = torch.randn(intermediate_size, hidden, dtype=dtype, device=device)\n",
        "b_fc1 = torch.randn(intermediate_size, 1,      dtype=dtype, device=device)\n",
        "W_fc2 = torch.randn(hidden, intermediate_size, dtype=dtype, device=device)\n",
        "b_fc2 = torch.randn(hidden, 1,                 dtype=dtype, device=device)\n",
        "\n",
        "# position embedding\n",
        "max_pos = sequence_length\n",
        "pos_emb = torch.randn(max_pos, hidden, dtype=dtype, device=device)  # [T, hidden]\n",
        "\n",
        "# layer norm\n",
        "ln1 = nn.LayerNorm(hidden, eps=1e-5, elementwise_affine=True).to(device)\n",
        "ln2 = nn.LayerNorm(hidden, eps=1e-5, elementwise_affine=True).to(device)\n",
        "\n",
        "# scale\n",
        "scale = 1.0 / math.sqrt(head_dim)"
      ],
      "metadata": {
        "id": "s2Tc16xY_Jqs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common = torch.randn(1, sequence_length, hidden)"
      ],
      "metadata": {
        "id": "TixgMoxv8dXn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cO8QCQjTMT-",
        "outputId": "07655687-5cf7-47c9-9f1f-e3d5fdef41f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2048\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "# KV cache\n",
        "K_cache = torch.zeros(sequence_length, hidden, dtype=dtype, device=device)  # time-major\n",
        "V_cache = torch.zeros(sequence_length, hidden, dtype=dtype, device=device)\n",
        "\n",
        "final_output = []\n",
        "\n",
        "def right_mm_colvec(W: torch.Tensor, x_col: torch.Tensor) -> torch.Tensor:\n",
        "    # W: [C, C], x_col: [C, 1] -> (x_col^T @ W^T)^T = [C,1]\n",
        "    return (x_col.T @ W.T).T\n",
        "\n",
        "\n",
        "for t in range(sequence_length):\n",
        "    # input = input token + position embedding\n",
        "    x_input = common[:, t, :].T + pos_emb[t].view(-1, 1)  # [hidden,1]\n",
        "\n",
        "    #  Self-Attention (pre-norm)\n",
        "    x1 = ln1(x_input.view(1, 1, hidden)).view(hidden, 1) # CHANGED: Attention ì „ RMSNorm\n",
        "\n",
        "    # Q/K/V projection + bias\n",
        "    q = right_mm_colvec(W_q, x1) + b_q      # [hidden,1]\n",
        "    k = right_mm_colvec(W_k, x1) + b_k\n",
        "    v = right_mm_colvec(W_v, x1) + b_v\n",
        "\n",
        "    # store K & V values in KV cache\n",
        "    K_cache[t, :] = k.view(-1)\n",
        "    V_cache[t, :] = v.view(-1)\n",
        "\n",
        "    # reshape to per-head\n",
        "    # q: [H, 1, D], k_all: [H, D, t+1], v_all: [H, t+1, D]\n",
        "    q_h = q.view(num_head, head_dim, 1).transpose(1, 2)                    # [H,1,D]\n",
        "    k_all = K_cache[:t+1, :].view(t+1, num_head, head_dim).transpose(0,1)  # [H,t+1,D]\n",
        "    v_all = V_cache[:t+1, :].view(t+1, num_head, head_dim).transpose(0,1)  # [H,t+1,D]\n",
        "\n",
        "    attn_scores = torch.matmul(q_h, k_all.transpose(-2, -1)) * scale  # [H,1,t+1]\n",
        "    attn_probs  = F.softmax(attn_scores, dim=-1)                      # causal: limit the length to t+1\n",
        "\n",
        "    context_h = torch.matmul(attn_probs, v_all)        # [H, 1, t+1] @ [H, t+1, D] = [H,1,D]\n",
        "    context = context_h.transpose(1, 2).contiguous().view(hidden, 1)  # [hidden,1]\n",
        "\n",
        "    # out projection + bias\n",
        "    attn_out = right_mm_colvec(W_o, context) + b_o  # [hidden,1]\n",
        "\n",
        "\n",
        "    # residual connection\n",
        "    x_attn = x_input + attn_out\n",
        "\n",
        "    #  MLP (pre-norm)\n",
        "    x2 = ln2(x_attn.view(1, 1, hidden)).view(hidden, 1)\n",
        "\n",
        "    hidden_mid = right_mm_colvec(W_fc1, x2) + b_fc1      # [3072,1]\n",
        "    hidden_mid = F.gelu(hidden_mid)\n",
        "    mlp_out    = right_mm_colvec(W_fc2, hidden_mid) + b_fc2   # [768,1]\n",
        "    # residual connection\n",
        "    x_out = x_attn + mlp_out\n",
        "    final_output.append(x_out)\n",
        "\n",
        "print(len(final_output))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stack: [2048, 768, 1]\n",
        "stacked = torch.stack(final_output, dim=0)\n",
        "stacked = stacked.squeeze(-1)\n",
        "verilog_out = stacked.unsqueeze(0)\n",
        "\n",
        "print(verilog_out.shape)  # torch.Size([1, 2048, 768])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSE_TaFy-nnI",
        "outputId": "3a596437-8125-4bbc-e27f-b1b7aeae2240"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2048, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OPTAttention(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "        num_attention_heads = 12\n",
        "        head_dim = 64\n",
        "        hidden_size = 768\n",
        "        dropout_prob=0.1\n",
        "        self.num_heads = num_attention_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.embed_dim = hidden_size\n",
        "        self.scale = 1 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(self.embed_dim, 3 * self.embed_dim)\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None,\n",
        "        past_key: torch.Tensor = None,\n",
        "        past_value: torch.Tensor = None,\n",
        "    ):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.view(B, T, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "\n",
        "        q = q.transpose(1, 2)  # [B, heads, T, head_dim]\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Concatenate cached K/V\n",
        "        if past_key is not None:\n",
        "            k = torch.cat([past_key, k], dim=-2)\n",
        "            v = torch.cat([past_value, v], dim=-2)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores + attention_mask\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        #attn_probs = self.dropout(attn_probs)\n",
        "\n",
        "        context = torch.matmul(attn_probs, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(context)\n",
        "        #out = self.dropout(out)\n",
        "\n",
        "        return out, k, v  # return updated cache\n",
        "\n",
        "\n",
        "class OPTMLP(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        hidden_size = 768\n",
        "        intermediate_size = 3072\n",
        "        dropout_prob=0.1\n",
        "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "self_attn = OPTAttention()\n",
        "self_mlp = OPTMLP()"
      ],
      "metadata": {
        "id": "Ju1AueC3DRWJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the weights & biases\n",
        "self_attn.qkv_proj.weight.data = torch.cat([W_q, W_k, W_v], dim=0)\n",
        "self_attn.qkv_proj.bias.data = torch.cat([b_q, b_k, b_v], dim=0).squeeze()\n",
        "\n",
        "self_attn.out_proj.weight.data = W_o\n",
        "self_attn.out_proj.bias.data = b_o.squeeze()\n",
        "\n",
        "self_mlp.fc1.weight.data=W_fc1\n",
        "self_mlp.fc1.bias.data=b_fc1.squeeze()\n",
        "self_mlp.fc2.weight.data=W_fc2\n",
        "self_mlp.fc2.bias.data=b_fc2.squeeze()"
      ],
      "metadata": {
        "id": "p0vvUEcZFwpg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T=2048\n",
        "x=common + pos_emb.unsqueeze(0)\n",
        "causal_bool = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "attention_mask = torch.zeros(1, 1, T, T, device=x.device, dtype=x.dtype)\n",
        "attention_mask = attention_mask.masked_fill(causal_bool, float(\"-inf\"))\n",
        "\n",
        "residual = x\n",
        "x = ln1(x)\n",
        "x, new_key, new_value = self_attn(x, attention_mask, None, None)\n",
        "x = x + residual\n",
        "\n",
        "residual = x\n",
        "x =ln2(x)\n",
        "x = self_mlp(x)\n",
        "x = x + residual\n",
        "pytorch_out = x"
      ],
      "metadata": {
        "id": "l3RWi_S6DGwp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff = (verilog_out - pytorch_out).abs()\n",
        "\n",
        "print(\"max abs diff:\", diff.max().item())\n",
        "print(\"mean abs diff:\", diff.mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNzPYdgVLkM3",
        "outputId": "6a34d671-9798-41d9-f28c-c169bab82d21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max abs diff: 0.978271484375\n",
            "mean abs diff: 0.005575356539338827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# absolute / relative difference\n",
        "abs_diff = (verilog_out - pytorch_out).abs()\n",
        "rel_diff = abs_diff / (pytorch_out.abs() + 1e-12)\n",
        "\n",
        "# find the coordinate where maximum relative difference occurs\n",
        "max_rel_val, max_rel_idx = torch.max(rel_diff.view(-1), dim=0)\n",
        "max_rel_coords = torch.unravel_index(max_rel_idx, rel_diff.shape)\n",
        "\n",
        "# get the values\n",
        "v_val = verilog_out[max_rel_coords].item()\n",
        "p_val = pytorch_out[max_rel_coords].item()\n",
        "abs_val = abs_diff[max_rel_coords].item()\n",
        "\n",
        "print(\"Max relative diff:\", max_rel_val.item())\n",
        "print(\"Index (B, T, C):\", tuple(c.item() for c in max_rel_coords))\n",
        "print(\"verilog_out value:\", v_val)\n",
        "print(\"pytorch_out value:\", p_val)\n",
        "print(\"abs diff:\", abs_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jT-LPwWMcae",
        "outputId": "c322f135-e6d3-46bd-a314-e4afe5e3775f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max relative diff: 18.45945930480957\n",
            "Index (B, T, C): (0, 1928, 470)\n",
            "verilog_out value: 0.0394287109375\n",
            "pytorch_out value: -0.00225830078125\n",
            "abs diff: 0.04168701171875\n"
          ]
        }
      ]
    }
  ]
}